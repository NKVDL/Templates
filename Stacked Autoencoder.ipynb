{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Jul 16 16:56:49 2018\n",
    "\n",
    "@author: NVDL\n",
    "\"\"\"\n",
    "###Part 1 - Importing \n",
    "#Importing the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the dataset\n",
    "movies = pd.read_csv('/Users/NVDL/Code/Practice_/Data/Math/Ranking/Movies/ml-1m/movies.dat',\n",
    "                     sep='::',\n",
    "                     header= None, \n",
    "                     engine = 'python', \n",
    "                     encoding ='latin-1')\n",
    "\n",
    "users = pd.read_csv('/Users/NVDL/Code/Practice_/Data/Math/Ranking/Movies/ml-1m/users.dat',\n",
    "                     sep='::',\n",
    "                     header= None, \n",
    "                     engine = 'python', \n",
    "                     encoding ='latin-1')\n",
    "\n",
    "ratings = pd.read_csv('/Users/NVDL/Code/Practice_/Data/Math/Ranking/Movies/ml-1m/ratings.dat',\n",
    "                     sep='::',\n",
    "                     header= None, \n",
    "                     engine = 'python', \n",
    "                     encoding ='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Part 2 - Preprocessing train/test set\n",
    "#Preparing the training set \n",
    "training_set = pd.read_csv('/Users/NVDL/Code/Practice_/Data/Math/Ranking/Movies/ml-100k/u1.base',\n",
    "                           delimiter = '\\t') #80% of total set \n",
    "#Convert df training_set to array\n",
    "training_set = np.array(training_set, dtype = 'int')\n",
    "\n",
    "#Preparing the the test set\n",
    "test_set = pd.read_csv('/Users/NVDL/Code/Practice_/Data/Math/Ranking/Movies/ml-100k/u1.test',\n",
    "                           delimiter = '\\t') #20% of total set \n",
    "#Convert df test_set to array\n",
    "test_set = np.array(test_set, dtype = 'int')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Add variable to confirm # of customers and # of movies. To do this we add the \n",
    "maximum values of the columns of the training_set and test_set. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the number of users and movies\n",
    "num_users = int(max(max(training_set[:,0]), max(test_set[:,0]))) #first column, maximum user id\n",
    "num_movies = int(max(max(training_set[:,1]), max(test_set[:,1]))) #first column, no. of movies\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Which yields:\n",
    "    \n",
    "max(training_set[:,0] = 943 \n",
    "max(test_set[:,0]) = 462\n",
    "max(training_set[:,1]) = 1682\n",
    "max(test_set[:,1]) = 1591 \n",
    "\n",
    "num_users = 943\n",
    "num_movies = 1682 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting data into array with users in lines \n",
    "#and movies in columns (943 list of users with list of 1682 movies usersrating)\n",
    "def convert(data):\n",
    "    new_data = []\n",
    "    for id_users in range(1, num_users + 1): #1, up to [shift data 1+] num_users \n",
    "        id_movies = data[:,1][data[:,0] == id_users] #movie of users\n",
    "        id_ratings = data[:,2][data[:,0] == id_users] #movie rating of users\n",
    "        ratings = np.zeros(num_movies)\n",
    "        ratings[id_movies - 1] = id_ratings \n",
    "        new_data.append(list(ratings))\n",
    "    return new_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[        1,         2,         3, 876893171],\n",
       "       [        1,         3,         4, 878542960],\n",
       "       [        1,         4,         3, 876893119],\n",
       "       ...,\n",
       "       [      943,      1188,         3, 888640250],\n",
       "       [      943,      1228,         3, 888640275],\n",
       "       [      943,      1330,         3, 888692465]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[        1,        10,         3, 875693118],\n",
       "       [        1,        12,         5, 878542960],\n",
       "       [        1,        14,         5, 874965706],\n",
       "       ...,\n",
       "       [      459,       934,         3, 879563639],\n",
       "       [      460,        10,         3, 882912371],\n",
       "       [      462,       682,         5, 886365231]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert training set to return list of customers with lists of rating per movie\n",
    "training_set = convert(training_set)\n",
    "test_set = convert(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the dataset into Torch tensors, multidimensional tensors\n",
    "training_set = torch.FloatTensor(training_set)\n",
    "test_set = torch.FloatTensor(test_set)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Part 3 - Creating architecture of Neural Network\n",
    "class SAE(nn.Module):\n",
    "    def __init__(self, ): #Self = object of SAE\n",
    "        super(SAE, self).__init__() #Super calls every inheritance modules from SAE\n",
    "        self.fc1 = nn.Linear(num_movies, 20) #Input vector and first encoded vector of 20 elements for 1st hidden layers\n",
    "        \"\"\"\n",
    "        If input is related to one of the encoded vectors in the hidden layer,\n",
    "        it will activate that node.\n",
    "        \"\"\"\n",
    "        self.fc2 = nn.Linear(20, 10) #Input from previous output, and new input to second hidden layer\n",
    "        \n",
    "        \"\"\"\n",
    "        Let's reconstruct our model from the hidden layers\n",
    "        \"\"\"\n",
    "        \n",
    "        self.fc3 = nn.Linear(10, 20)\n",
    "        self.fc4 = nn.Linear(20, num_movies)\n",
    "        \n",
    "        \"\"\"\n",
    "        Let's create activation function\n",
    "        \"\"\"\n",
    "        self.activation =nn.Sigmoid() #Sigmoid activation for the full network\n",
    "\n",
    "    \n",
    "    def forward(self, x): #Rating of movies per user\n",
    "        \"\"\"\n",
    "        #x = activation of nodes in the input layer fc1 that converts 20 input\n",
    "        nodes to 10 output nodes in the hidden layer, and uses activation function\n",
    "        to encode input vector x. \n",
    "        \"\"\"\n",
    "        x = self.activation(self.fc1(x)) #Input to 20 elements\n",
    "        x = self.activation(self.fc2(x)) #Input 20 elements to 10 elements\n",
    "        \n",
    "        #Reconstruct\n",
    "        x = self.activation(self.fc3(x)) #Input 10 elements to 20 elements\n",
    "        x = self.fc4(x) #Vector of predicted ratings\n",
    "        \n",
    "        return x \n",
    "    \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Create an object of class with a learning rate of 0.01 and a weight_decay of 0.5 to reduce learning rate after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae = SAE() \n",
    "criterion = nn.MSELoss() #Object of sae \n",
    "optimizer = optim.RMSprop(sae.parameters(), lr=0.01, weight_decay=0.5) #all features in our architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1loss: 1.770976260699006\n",
      "epoch: 2loss: 1.0966909778060805\n",
      "epoch: 3loss: 1.0536440186600466\n",
      "epoch: 4loss: 1.0383528789871033\n",
      "epoch: 5loss: 1.0310357578805416\n",
      "epoch: 6loss: 1.0265327117832463\n",
      "epoch: 7loss: 1.0240013224652358\n",
      "epoch: 8loss: 1.0218246776954192\n",
      "epoch: 9loss: 1.0208632993150688\n",
      "epoch: 10loss: 1.019559880811075\n",
      "epoch: 11loss: 1.0188678294418307\n",
      "epoch: 12loss: 1.0182290285752373\n",
      "epoch: 13loss: 1.0180218666973364\n",
      "epoch: 14loss: 1.017370061074304\n",
      "epoch: 15loss: 1.0170723034538427\n",
      "epoch: 16loss: 1.0168677461742364\n",
      "epoch: 17loss: 1.0168903566913232\n",
      "epoch: 18loss: 1.0161682702519792\n",
      "epoch: 19loss: 1.0164777885690595\n",
      "epoch: 20loss: 1.0162646870058751\n",
      "epoch: 21loss: 1.0161942278498992\n",
      "epoch: 22loss: 1.0157987575012628\n",
      "epoch: 23loss: 1.0160014347697466\n",
      "epoch: 24loss: 1.0157687362230634\n",
      "epoch: 25loss: 1.0158070378226538\n",
      "epoch: 26loss: 1.01547029865176\n",
      "epoch: 27loss: 1.0154456728306465\n",
      "epoch: 28loss: 1.0149542365302469\n",
      "epoch: 29loss: 1.0128178719735959\n",
      "epoch: 30loss: 1.0111420272347782\n",
      "epoch: 31loss: 1.0100347962110474\n",
      "epoch: 32loss: 1.0070804657320198\n",
      "epoch: 33loss: 1.0072798351953642\n",
      "epoch: 34loss: 1.0030105520380561\n",
      "epoch: 35loss: 1.0023248828699578\n",
      "epoch: 36loss: 0.9986215218851978\n",
      "epoch: 37loss: 0.9959918626495279\n",
      "epoch: 38loss: 0.9959583673537107\n",
      "epoch: 39loss: 0.9947863063999985\n",
      "epoch: 40loss: 0.9908760142266835\n",
      "epoch: 41loss: 0.9905221435779444\n",
      "epoch: 42loss: 0.9872813709030143\n",
      "epoch: 43loss: 0.9871544206295895\n",
      "epoch: 44loss: 0.9834398877183996\n",
      "epoch: 45loss: 0.98627421249329\n",
      "epoch: 46loss: 0.9824708924311281\n",
      "epoch: 47loss: 0.9814908335251249\n",
      "epoch: 48loss: 0.97743464336183\n",
      "epoch: 49loss: 0.9837408066319597\n",
      "epoch: 50loss: 0.9806248808271767\n",
      "epoch: 51loss: 0.9797957317981572\n",
      "epoch: 52loss: 0.9739134290221769\n",
      "epoch: 53loss: 0.9806982419603129\n",
      "epoch: 54loss: 0.9777249618951418\n",
      "epoch: 55loss: 0.9777917847901805\n",
      "epoch: 56loss: 0.9756023998209565\n",
      "epoch: 57loss: 0.9763952486987163\n",
      "epoch: 58loss: 0.9731315121006917\n",
      "epoch: 59loss: 0.9701032431036444\n",
      "epoch: 60loss: 0.9652610075931736\n",
      "epoch: 61loss: 0.9661922550947678\n",
      "epoch: 62loss: 0.9617210119815771\n",
      "epoch: 63loss: 0.9633088055883083\n",
      "epoch: 64loss: 0.9619570735640791\n",
      "epoch: 65loss: 0.960924353971903\n",
      "epoch: 66loss: 0.9597498033308745\n",
      "epoch: 67loss: 0.9619406524877294\n",
      "epoch: 68loss: 0.9552567530138116\n",
      "epoch: 69loss: 0.9556852330312595\n",
      "epoch: 70loss: 0.9527907975611094\n",
      "epoch: 71loss: 0.9577505884886571\n",
      "epoch: 72loss: 0.953491730524633\n",
      "epoch: 73loss: 0.9559602201973393\n",
      "epoch: 74loss: 0.9500910236991961\n",
      "epoch: 75loss: 0.9532570824177428\n",
      "epoch: 76loss: 0.9484679425085623\n",
      "epoch: 77loss: 0.950624132578847\n",
      "epoch: 78loss: 0.9457857385573138\n",
      "epoch: 79loss: 0.947917613427934\n",
      "epoch: 80loss: 0.9446213523353625\n",
      "epoch: 81loss: 0.9465854391705334\n",
      "epoch: 82loss: 0.9436585924044827\n",
      "epoch: 83loss: 0.9446945663805291\n",
      "epoch: 84loss: 0.9423521590565271\n",
      "epoch: 85loss: 0.9438073195589056\n",
      "epoch: 86loss: 0.9408535562894527\n",
      "epoch: 87loss: 0.9452259430295239\n",
      "epoch: 88loss: 0.9390913486629636\n",
      "epoch: 89loss: 0.9401058817206767\n",
      "epoch: 90loss: 0.9388922236111029\n",
      "epoch: 91loss: 0.9407086597810493\n",
      "epoch: 92loss: 0.9385406414318255\n",
      "epoch: 93loss: 0.9399223542226133\n",
      "epoch: 94loss: 0.9379251319154859\n",
      "epoch: 95loss: 0.9393530521420178\n",
      "epoch: 96loss: 0.936235231959205\n",
      "epoch: 97loss: 0.9382352917430083\n",
      "epoch: 98loss: 0.9345612295931028\n",
      "epoch: 99loss: 0.9370416958206172\n",
      "epoch: 100loss: 0.9340089921222798\n",
      "epoch: 101loss: 0.9359904795666603\n",
      "epoch: 102loss: 0.9327384107298964\n",
      "epoch: 103loss: 0.9344456816732674\n",
      "epoch: 104loss: 0.9327351222843232\n",
      "epoch: 105loss: 0.9346649723746682\n",
      "epoch: 106loss: 0.9320323166948444\n",
      "epoch: 107loss: 0.9339059997076872\n",
      "epoch: 108loss: 0.9317001536667475\n",
      "epoch: 109loss: 0.9333652301227714\n",
      "epoch: 110loss: 0.9311097709690208\n",
      "epoch: 111loss: 0.932465957244938\n",
      "epoch: 112loss: 0.930327740161072\n",
      "epoch: 113loss: 0.9317335296957601\n",
      "epoch: 114loss: 0.9297130879671519\n",
      "epoch: 115loss: 0.9309893722066932\n",
      "epoch: 116loss: 0.9287964714688315\n",
      "epoch: 117loss: 0.9303200748291465\n",
      "epoch: 118loss: 0.9291377540760005\n",
      "epoch: 119loss: 0.9297829360609077\n",
      "epoch: 120loss: 0.9288145631130236\n",
      "epoch: 121loss: 0.929097222014962\n",
      "epoch: 122loss: 0.9281560727214639\n",
      "epoch: 123loss: 0.9284416336109874\n",
      "epoch: 124loss: 0.9274505209249039\n",
      "epoch: 125loss: 0.9278732196084334\n",
      "epoch: 126loss: 0.9268153205962001\n",
      "epoch: 127loss: 0.9276431730555063\n",
      "epoch: 128loss: 0.9264353121042677\n",
      "epoch: 129loss: 0.9272057609348404\n",
      "epoch: 130loss: 0.9259720994622511\n",
      "epoch: 131loss: 0.9266518427480553\n",
      "epoch: 132loss: 0.9254654004212389\n",
      "epoch: 133loss: 0.9260019162946311\n",
      "epoch: 134loss: 0.9248354658256569\n",
      "epoch: 135loss: 0.925304586318001\n",
      "epoch: 136loss: 0.9242609973874462\n",
      "epoch: 137loss: 0.9248891433793258\n",
      "epoch: 138loss: 0.9239540095342093\n",
      "epoch: 139loss: 0.9244597622727417\n",
      "epoch: 140loss: 0.9233905346016001\n",
      "epoch: 141loss: 0.9240283542578683\n",
      "epoch: 142loss: 0.9229833113967815\n",
      "epoch: 143loss: 0.9230917038230493\n",
      "epoch: 144loss: 0.9224495582588564\n",
      "epoch: 145loss: 0.922580979813549\n",
      "epoch: 146loss: 0.9218829890296644\n",
      "epoch: 147loss: 0.9223994590838627\n",
      "epoch: 148loss: 0.9217540481503406\n",
      "epoch: 149loss: 0.9221570577850366\n",
      "epoch: 150loss: 0.921069986421201\n",
      "epoch: 151loss: 0.9215571149473705\n",
      "epoch: 152loss: 0.9207136937415132\n",
      "epoch: 153loss: 0.9211811539527984\n",
      "epoch: 154loss: 0.9205337957579321\n",
      "epoch: 155loss: 0.9208806362937003\n",
      "epoch: 156loss: 0.9200623243777554\n",
      "epoch: 157loss: 0.92052199494017\n",
      "epoch: 158loss: 0.9194962683762345\n",
      "epoch: 159loss: 0.9198434155910021\n",
      "epoch: 160loss: 0.9191663041107219\n",
      "epoch: 161loss: 0.919592463482132\n",
      "epoch: 162loss: 0.9208145479550066\n",
      "epoch: 163loss: 0.919303334503631\n",
      "epoch: 164loss: 0.9186634518899939\n",
      "epoch: 165loss: 0.9190366048692162\n",
      "epoch: 166loss: 0.9184904793559062\n",
      "epoch: 167loss: 0.9184239917171788\n",
      "epoch: 168loss: 0.9178938714604268\n",
      "epoch: 169loss: 0.91818281986691\n",
      "epoch: 170loss: 0.9177628742921587\n",
      "epoch: 171loss: 0.9176136561210031\n",
      "epoch: 172loss: 0.9172723271095167\n",
      "epoch: 173loss: 0.9178636276929112\n",
      "epoch: 174loss: 0.9172255784079862\n",
      "epoch: 175loss: 0.9173182924432007\n",
      "epoch: 176loss: 0.916752360801997\n",
      "epoch: 177loss: 0.9170303731007085\n",
      "epoch: 178loss: 0.9163286174451057\n",
      "epoch: 179loss: 0.9166107389268324\n",
      "epoch: 180loss: 0.916480843685406\n",
      "epoch: 181loss: 0.9163965359955414\n",
      "epoch: 182loss: 0.9161130450591966\n",
      "epoch: 183loss: 0.9163592310386174\n",
      "epoch: 184loss: 0.9158198324862188\n",
      "epoch: 185loss: 0.9158753775073932\n",
      "epoch: 186loss: 0.915714772623156\n",
      "epoch: 187loss: 0.9155481987539972\n",
      "epoch: 188loss: 0.9152505884848535\n",
      "epoch: 189loss: 0.9153165473683812\n",
      "epoch: 190loss: 0.9152226482846683\n",
      "epoch: 191loss: 0.9151946962971729\n",
      "epoch: 192loss: 0.9150943703201709\n",
      "epoch: 193loss: 0.9147922501050854\n",
      "epoch: 194loss: 0.91471045605222\n",
      "epoch: 195loss: 0.9147605283714341\n",
      "epoch: 196loss: 0.9146753897395213\n",
      "epoch: 197loss: 0.9142606231449452\n",
      "epoch: 198loss: 0.9139397158861365\n",
      "epoch: 199loss: 0.9143055601225831\n",
      "epoch: 200loss: 0.9140289966742838\n"
     ]
    }
   ],
   "source": [
    "#Part 4 - Training & Testing SAE Model\n",
    "epochs = 200 \n",
    "for epoch in range(1,epochs+1): #Upperbound excluded in python\n",
    "    #Initialise values that keeps information over epochs\n",
    "    training_loss = 0 #at beginning of training\n",
    "    #exclude variables without ratings, only users with > 1 ratings\n",
    "    s= 0. #RMS is a float\n",
    "    #let's get input vector of features per user\n",
    "    for id_user in range(num_users):\n",
    "        \"\"\"\n",
    "        We need to add another dimension corresponding to the batches, else \n",
    "        Pytorch, Keras etc. won't take the var in. \n",
    "        \"\"\"\n",
    "        input = Variable(training_set[id_user]).unsqueeze(0) #Correspond with index of user in the loop\n",
    "        #Clone input\n",
    "        target = input.clone() #A clone of ratings per user\n",
    "        #If sum  observation > 0, thenn users rated at least 1 rating, \n",
    "        if torch.sum(target.data > 0) > 0:\n",
    "            output = sae(input) #Applying object with real ratings to class SAE\n",
    "            target.require_grad = False #we don't compute gradient with respect to target\n",
    "            #Only include non-zero values for target\n",
    "            output[target ==0] = 0 #Will not be included in rmsprop optimiser, for large datasets\n",
    "            #Calculate loss \n",
    "            loss = criterion(output, target) #Output, vs  real non-zero ratings for movie per user\n",
    "            \"\"\"\n",
    "            Average of error by movies that have non-zero ratings. \n",
    "            \"\"\"\n",
    "            mean_corrector = num_movies/float(torch.sum(target.data > 0) + 1e-10) \n",
    "            #Decrease loss\n",
    "            loss.backward() #Which direction weights are updated\n",
    "            #\n",
    "            training_loss += np.sqrt(loss.item()*mean_corrector) \n",
    "            #Increment S by 1 per rated movie by user\n",
    "            s += 1.\n",
    "            #Optimizer to update the weights\n",
    "            optimizer.step()  #Amount by which weights are updated\n",
    "    #Print epoch,loss after every epoch\n",
    "    print ('epoch: '+str(epoch)+ 'loss: '+str(training_loss/s)) #Loss of < 1 = optimal\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.9496335469083546\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0 #at beginning of testing\n",
    "s= 0. #RMS is a float\n",
    "#let's get input vector of features per user\n",
    "for id_user in range(num_users):\n",
    "    \"\"\"\n",
    "    We need to add another dimension corresponding to the batches, else \n",
    "    Pytorch, Keras etc. won't take the var in. \n",
    "    \"\"\"\n",
    "    input = Variable(training_set[id_user]).unsqueeze(0) #Use training_set to compare loss with real examples\n",
    "    #Test set with real ratings\n",
    "    target = Variable(test_set[id_user]).unsqueeze(0)\n",
    "    if torch.sum(target.data > 0) > 0:#If sum  observation > 0, then users rated at least 1 rating \n",
    "        output = sae(input) #Applying object with real ratings to class SAE\n",
    "        target.require_grad = False #we don't compute gradient with respect to target\n",
    "        #Only include non-zero values for target\n",
    "        output[target ==0] = 0 #Will not be included in rmsprop optimiser, for large datasets,thus no zeros included\n",
    "        #Calculate loss \n",
    "        loss = criterion(output, target) #Output, vs  real non-zero ratings for movie per user\n",
    "        \"\"\"\n",
    "        Average of error by movies that have non-zero ratings. Adjusting factor\n",
    "        testing those with users|rating  > 1. Or rated at least n movie > 1. \n",
    "        \"\"\"\n",
    "        mean_corrector = num_movies/float(torch.sum(target.data > 0) + 1e-10) \n",
    "        #Loss of new data\n",
    "        test_loss += np.sqrt(loss.item()*mean_corrector) \n",
    "        #Increment S by 1 per rated movie by user\n",
    "        s += 1.\n",
    "        \n",
    "#Print epoch loss after every epoch\n",
    "print ('test loss: '+str(test_loss/s)) #Loss of < 1 = optimal"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We obtain a training_loss of .91, test_loss of .95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
